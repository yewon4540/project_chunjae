{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 편의를 위해 함수 이름은 ```def (내용)_check():``` 모양으로 작성하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식\n",
    "import re\n",
    "\n",
    "# 특수문자(이모티콘)제거\n",
    "from emoji import core # 정규식으로 없어지지 않는 특수문자 제거\n",
    "\n",
    "# 형태소 분석기\n",
    "from konlpy.tag import Kkma, Hannanum, Mecab, Okt # Mecab은 error발생으로 linux환경에서 실행 필요.\n",
    "\n",
    "# 형태소 분석기 : Komoran\n",
    "from PyKomoran import Komoran, DEFAULT_MODEL # Komoran error는 조금 더 살펴보겠음...\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kkma = Kkma()\n",
    "Hannanum = Hannanum()\n",
    "Okt = Okt() # 구 Twitter\n",
    "komoran = Komoran(DEFAULT_MODEL['FULL']) # FULL or LIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-d %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\MeCab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0m_MeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTagger_swiginit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32848\\2661114201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mMecab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMecab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_mecab.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s/data/tagset/mecab.json'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstallpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The MeCab dictionary does not exist at \"%s\". Is the dictionary correctly installed?\\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab(\\'/some/dic/path\\')\"'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdicpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Install MeCab in order to use it: http://konlpy.org/en/latest/install/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: The MeCab dictionary does not exist at \"/usr/local/lib/mecab/dic/mecab-ko-dic\". Is the dictionary correctly installed?\nYou can also try entering the dictionary path when initializing the Mecab class: \"Mecab('/some/dic/path')\""
     ]
    }
   ],
   "source": [
    "Mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>click</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어떤게나을까요?! 지금은 웅진 학습지로만(지면) 하고있어요~ 패드로 슬슬 갈까하는데...</td>\n",
       "      <td>2023-03-03 14:49:00</td>\n",
       "      <td>502</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어떤게나을까요?! 지금은 웅진 학습지로만(지면) 하고있어요~ 패드로 슬슬 갈까하는데...</td>\n",
       "      <td>2023-03-03 14:49:00</td>\n",
       "      <td>502</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>초1이고 밀크티 하고 있어요\\n오늘의 학습만 매일하고 있는데요\\n초등 2학년때까진 ...</td>\n",
       "      <td>2023-03-20 22:35:00</td>\n",
       "      <td>208</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대부분의 받아쓰기는 빠르면 4살, 늦어도\\n초등학교 입학전부터 학습하는 경우가 많잖...</td>\n",
       "      <td>2022-09-06 16:58:00</td>\n",
       "      <td>661</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>초1인데 패드로 하는 학습지를 시켜보고 싶은데요.\\n빨간펜, 윙크,밀크티 중 어떤게...</td>\n",
       "      <td>2023-03-14 15:02:00</td>\n",
       "      <td>219</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                date  \\\n",
       "0  어떤게나을까요?! 지금은 웅진 학습지로만(지면) 하고있어요~ 패드로 슬슬 갈까하는데... 2023-03-03 14:49:00   \n",
       "1  어떤게나을까요?! 지금은 웅진 학습지로만(지면) 하고있어요~ 패드로 슬슬 갈까하는데... 2023-03-03 14:49:00   \n",
       "2  초1이고 밀크티 하고 있어요\\n오늘의 학습만 매일하고 있는데요\\n초등 2학년때까진 ... 2023-03-20 22:35:00   \n",
       "3  대부분의 받아쓰기는 빠르면 4살, 늦어도\\n초등학교 입학전부터 학습하는 경우가 많잖... 2022-09-06 16:58:00   \n",
       "4  초1인데 패드로 하는 학습지를 시켜보고 싶은데요.\\n빨간펜, 윙크,밀크티 중 어떤게... 2023-03-14 15:02:00   \n",
       "\n",
       "   click topic  \n",
       "0    502    초1  \n",
       "1    502    초1  \n",
       "2    208    초1  \n",
       "3    661    초1  \n",
       "4    219    초1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.read_parquet('./data/crwaling_data4.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어떤게나을까요?! 지금은 웅진 학습지로만(지면) 하고있어요~ 패드로 슬슬 갈까하는데 고학년까지 과학이랑 영어같이하면 구몬이 나을지,밀크티가 나을지 모르겠어요~ 비교좀 부탁드려요~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리 과정\n",
    "\n",
    "<details>\n",
    "<summary>내용</summary>\n",
    "<div markdown=\"1\">       \n",
    "1. 태그 제거\n",
    "    - `http`, `제목 : ` 등의 규칙적이게 나타나는 값 제거\n",
    "\n",
    "1. 특수문자 제거\n",
    "    - 정규식(re)\n",
    "    - 이모티콘(emoji)\n",
    "\n",
    "1. 문자 통일\n",
    "    - 영어 대소문자\n",
    "    - 동음이의어 및 같은 의미를 담는 문자\n",
    "        - ex) 초2, 초등2학년, 초등학교 2학년\n",
    "        - ex) 밀크티, 밀크T, 밀ㅋㅌ, ㅁㅋㅌ, 밀*티, MilkT\n",
    "\n",
    "1. 불용어 제거\n",
    "    - stop_words\n",
    "        - mecab에는 불용어사전 기능이 존재함.\n",
    "        - 다른 형태소분석기의 경우에는 `stopwords`리스트와 `if not in` 을 활용\n",
    "        - 인터넷에 한국어 stop_words를 공유하는 경우가 있으므로 활용\n",
    "\n",
    "1. 특정 문자 존재 여부\n",
    "    - Keyword : `밀크티`, `홈런`, `태블릿` 등등...\n",
    "\n",
    "1. 토큰화\n",
    "    - Konlpy\n",
    "\n",
    "1. 표제어 추출\n",
    "    - LDA등의 모델 활용\n",
    "\n",
    "1. 벡터화\n",
    "    - numpy를 사용하여 벡터화 - 모델링에 활용\n",
    "\n",
    "1. 글자 수\n",
    "    - 글자수가 너무 긴 경우 : 내용이 부적합할 수 있다. (ex. 해피빈 모금함 후원자명단 이라는 글이 크롤링 되었는데, 아이디와 닉네임을 나열한 글이라서 더미데이터이면서 글자 수가 매우 길었다.)\n",
    "    - 글자수가 너무 짧은 경우 : 내용이 담겨있지 않을 수 있다. (ex. 밀크티 저는 비추천이요)\n",
    "\n",
    "1. 글자 형태\n",
    "    - 같은 문장이 여러번 쓰이는 경우 : 광고성 글일 가능성이 있다. (XX동 XX학원 XXX선생입니다. 저희 학원에서는~)\n",
    "\n",
    "1. 품사 태깅\n",
    "    - Konlpy.pos를 통해 불필요한 품사(조사 등)을 제거\n",
    "        - 체언(명사,대명사, 수사)만 남기고 제거, 필요에 따라 용언(중 동사, 형용사)도 남김.\n",
    "\n",
    "1. 띄어쓰기 구분\n",
    "    - 한글, 특히 온라인에 게시된 글들은 띄어쓰기가 올바르게 되어있지 않을 확률이 높기에 구분 짓는 방법 필요\n",
    "        - Mecab이나 Hannanum등의 형태소 분석기는 띄어쓰기를 잘 구분하지 못한다.\n",
    "\n",
    "1. 분석에 불필요한 문장 제거\n",
    "    - ?!?!?!?!?!?!?! - 해봐야 알 듯 해서 아직 미작성\n",
    "\n",
    "1. 낚시성 글 제거\n",
    "    - 이 프로젝트의 경우 크롤링 대상이 카페에서 이루어지므로 낚시성 글일 확률이 낮고, 주관성이 들어간 글들을 수집하기에 이 단계는 진행하지 않는다.\n",
    "\n",
    "1. 기준 설정\n",
    "    1. 기간 : 최근 n개월~\n",
    "    1. 광고 : \n",
    "        - 광고 시 들어가는 문구인 `본 포스팅(이 글)은 해당 업체로부터 제품&원고료를 지원받아 작성한 리뷰입니다.`의 존재 여부 확인\n",
    "            - 이미지로 존재할 경우, 이미지 text화(library : tesseract)를 통해 확인\n",
    "\n",
    "    1. 키워드 : \n",
    "        1. 검색 시 키워드 설정\n",
    "            - 초등 `@@@` :` @@@` = [`태블릿`, `온라인 학습`, `자기주도학습`, `선행학습`, ...]\n",
    "            - `@@@` 밀크티 : `@@@` = [`초등`, `중등`, ...]\n",
    "\n",
    "        1. 분석&시각화 키워드 설정\n",
    "            - 주제에 부합한 키워드(하이퍼파라미터.. HP) 설정\n",
    "                - `컨텐츠`가 주제일 경우 HP를 ['게임', '독서', ...]로 설정\n",
    "        \n",
    "        1. 중요도 & 긍/부정 여부\n",
    "            - 중요도 : LDA등의 토픽 모델\n",
    "            - 긍/부정 : Sentiment_Analysis\n",
    "            - 아직 실습해보지 못해서 잘 모름.\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tag제거\n",
    "    - 'http'나 text에서 규칙적으로 나오는 불필요한 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_tag_check(text):\n",
    "    text = re.sub('http:', '', text)\n",
    "    text = re.sub('comhttpsm.', '', text)\n",
    "    text = re.sub(r'(\\d{2,3})-(\\d{3,4})-?(\\d{0,4})?', '', text) # 전화번호 제거\n",
    "    text = re.sub('blog.', '', text)\n",
    "    text = re.sub('naver.', '', text)\n",
    "    text = re.sub('co. kr', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 특수문자 제거\n",
    "    - 정규식(re)\n",
    "    - 이모티콘(emoji)\n",
    "    - 자음만 존재하는 경우\n",
    "        - ex. 'ㅎㅎ맞아요'\n",
    "        - but 인터넷 텍스트에서는 자음으로 줄임말을 쓰는 경우가 있으니, 2단계(문자 통일)이후 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>click</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>어떤게나을까요 지금은 웅진 학습지로만지면 하고있어요 패드로 슬슬 갈까하는데 고학년까...</td>\n",
       "      <td>2023-03-03 14:49:00</td>\n",
       "      <td>502</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어떤게나을까요 지금은 웅진 학습지로만지면 하고있어요 패드로 슬슬 갈까하는데 고학년까...</td>\n",
       "      <td>2023-03-03 14:49:00</td>\n",
       "      <td>502</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>초1이고 밀크티 하고 있어요오늘의 학습만 매일하고 있는데요초등 2학년때까진 이걸로 ...</td>\n",
       "      <td>2023-03-20 22:35:00</td>\n",
       "      <td>208</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대부분의 받아쓰기는 빠르면 늦어도초등학교 입학전부터 학습하는 경우가 많잖아요.하지만...</td>\n",
       "      <td>2022-09-06 16:58:00</td>\n",
       "      <td>661</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>초1인데 패드로 하는 학습지를 시켜보고 싶은데요.빨간펜 윙크밀크티어떤게 좋을까요빨간...</td>\n",
       "      <td>2023-03-14 15:02:00</td>\n",
       "      <td>219</td>\n",
       "      <td>초1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                date  \\\n",
       "0  어떤게나을까요 지금은 웅진 학습지로만지면 하고있어요 패드로 슬슬 갈까하는데 고학년까... 2023-03-03 14:49:00   \n",
       "1  어떤게나을까요 지금은 웅진 학습지로만지면 하고있어요 패드로 슬슬 갈까하는데 고학년까... 2023-03-03 14:49:00   \n",
       "2  초1이고 밀크티 하고 있어요오늘의 학습만 매일하고 있는데요초등 2학년때까진 이걸로 ... 2023-03-20 22:35:00   \n",
       "3  대부분의 받아쓰기는 빠르면 늦어도초등학교 입학전부터 학습하는 경우가 많잖아요.하지만... 2022-09-06 16:58:00   \n",
       "4  초1인데 패드로 하는 학습지를 시켜보고 싶은데요.빨간펜 윙크밀크티어떤게 좋을까요빨간... 2023-03-14 15:02:00   \n",
       "\n",
       "   click topic  \n",
       "0    502    초1  \n",
       "1    502    초1  \n",
       "2    208    초1  \n",
       "3    661    초1  \n",
       "4    219    초1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'어떤게나을까요 지금은 웅진 학습지로만지면 하고있어요 패드로 슬슬 갈까하는데 고학년까지 과학이랑 영어같이하면 구몬이 나을지밀크티가 나을지 모르겠어요 비교좀 부탁드려요'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 특수 기호 제거\n",
    "df.text = df.text.apply(lambda x : re.sub('[-=+,#/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\n★▲;]','', x))\n",
    "\n",
    "# '\\E' 모양 제거\n",
    "df.text = df.text.apply(lambda x : re.sub('[\\a-zA-Z]. ','',x))\n",
    "\n",
    "# 이모티콘 제거\n",
    "df.text = df.text.apply(lambda x : core.replace_emoji(x, replace=''))\n",
    "\n",
    "# # 자음만 적힌 경우 제거\n",
    "# df.text = df.text.apply(lambda x : re.sub('[ㄱ-ㅎ]+', '', x))\n",
    "\n",
    "display(df.head())\n",
    "display(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이모티콘 실험이에용😊😊👍👍'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'이모티콘 실험이에용'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 이모티콘 제거 예시\n",
    "test_text = '이모티콘 실험이에용😊😊👍👍'\n",
    "\n",
    "# re 사용\n",
    "display(re.sub('[-=+,#/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》\\n★▲;]','', test_text))\n",
    "\n",
    "# emoji 사용\n",
    "display(core.replace_emoji(test_text, replace=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 문자 통일\n",
    "    - 영어 대소문자\n",
    "    - 동음이의어 및 같은 의미를 담는 문자 - `하나씩 추가 해야될 듯`\n",
    "        - ex) 초2, 초등2학년, 초등학교 2학년\n",
    "        - ex) 밀크티, 밀크T, 밀ㅋㅌ, ㅁㅋㅌ, 밀*티, MilkT\n",
    "\n",
    "        - 현재 문자통합 상황 : \n",
    "            `밀크티, 홈런`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 소문자화\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# 같은 의미를 담은 문자\n",
    "def combine_check(contents):\n",
    "\n",
    "    # 밀크티\n",
    "    contents = re.sub('밀크t', '밀크티', contents) #영어를 모두 소문자화 했으므로 대문자T는 진행 X\n",
    "    contents = re.sub('밀ㅋㅌ', '밀크티', contents)\n",
    "    contents = re.sub('ㅁㅋㅌ', '밀크티', contents)\n",
    "    contents = re.sub('밀*티', '밀크티', contents)\n",
    "    contents = re.sub('밀크*', '밀크티', contents)\n",
    "    contents = re.sub('*크티', '밀크티', contents)\n",
    "    contents = re.sub('밀**', '밀크티', contents)\n",
    "    contents = re.sub('*크*', '밀크티', contents)\n",
    "    contents = re.sub('**티', '밀크티', contents)\n",
    "    contents = re.sub('milkt', '밀크티', contents)\n",
    "\n",
    "    # 홈런\n",
    "    contents = re.sub('홈*', '홈런', contents)\n",
    "    contents = re.sub('*런', '홈런', contents)\n",
    "    contents = re.sub('아이스크림 홈런', '홈런', contents)\n",
    "    contents = re.sub('아이스크림 에듀', '홈런', contents)\n",
    "    contents = re.sub('홈런 홈런', '홈런', contents) # '아이스크림 에듀 홈런' 으로 쓰였을 경우 홈런이 두번...\n",
    "    contents = re.sub('ㅎㄹ', '홈런', contents)\n",
    "    contents = re.sub('ㅎㄹ', '홈런', contents)\n",
    "\n",
    "    # 명사\n",
    "    contents = re.sub('할미','할머니', contents)\n",
    "    contents = re.sub('티비','텔레비전', contents)\n",
    "    contents = re.sub('','', contents)\n",
    "    contents = re.sub('','', contents)\n",
    "    contents = re.sub('','', contents)\n",
    "\n",
    "    # contents = re.sub('','', contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 불용어 제거\n",
    "    - stop_words\n",
    "        - mecab에는 불용어사전 기능이 존재함.\n",
    "        - 다른 형태소분석기의 경우에는 `stopwords`리스트와 `if not in` 을 활용\n",
    "        - 인터넷에 한국어 stop_words를 공유하는 경우가 있으므로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 외의 분석기 (리스트는 예시)\n",
    "stop_words = ['아이', '맘', '거', '저', '저희', '것', '학모', '감사', '게', '때', '애', '원', '제', '개', '일', '듯', '만', '번', '데', '분', '건', '정도', '달', '정보', '이번', '부탁', '걸로', '후', '나', '부분', '걸', '저흰', '답변', '마음', '뼈', '건가요', '오늘', '왜', '시', '꺼', '친구', '마미', '때문', '하루', '명', '돈', '해요', '이', '가요', '기', '지금', '건가요', '반', '주세요', '편', '외', '소개', '입니다', '곳', '이거', '뭐', '땐', '건지', '그거', '점', '땐', '미', '풀', '대', '안', '눈', '공주', '사', '오', '백', '후', '나']\n",
    "\n",
    "def stopwords_check(A):\n",
    "    text_list = []\n",
    "    if A not in stop_words:\n",
    "        text_list.append(A)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 특정 문자 존재 여부\n",
    "    - Keyword : `밀크티`, `홈런`, `태블릿` 등등..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword가 text에 존재하는지 여부확인\n",
    "\n",
    "def keyword_check(keyword, text):\n",
    "    if keyword in text:\n",
    "        return text\n",
    "\n",
    "    else : \n",
    "        # none이나 ''이면 전처리 시 불편해서 우선은 이렇게 해뒀습니다.\n",
    "        return f'None keyword : {text}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 토큰화\n",
    "    - Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['어떤게나을까요', '지금은', '웅진', '학습지로만지면', '하고있어요', '패드로', '슬슬', '갈까하는데', '고학년까지', '과학이랑', '영어같이하면', '구몬이', '나을지밀크티가', '나을지', '모르겠어요', '비교좀', '부탁드려요']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = df['text'][0]\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 표제어 추출\n",
    "    - LDA등의 모델 활용\n",
    "    \n",
    "    `codefile 참조`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 벡터화\n",
    "    - numpy를 사용하여 벡터화 - 모델링에 활용\n",
    "\n",
    "모델링 파트에서<br>\n",
    "<code>\n",
    "np.arange(len(top_milk_nouns)) </code><br><br>혹은<br> <code>milk_indexes = df[df['cluster'] == 0].index<br>\n",
    "similarity = cosine_similarity(tfidf_vectors[milk_indexes[0]], tfidf_vectors[milk_indexes])<br>\n",
    "milk_1_similarity = np.sqrt(similarity.reshape(-1)[::-1]) <br> \n",
    "</code>\n",
    "같은 모양으로 각 모델에 맞게 벡터화 시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 글자 수\n",
    "    - 글자수가 너무 긴 경우 : 내용이 부적합 할 수 있다.\n",
    "        - ex. 크롤링한 데이터에 '해피빈 모금함 후원자 명단' 이라는 글이 있었는데, 아이디와 닉네임을 단순 나열한 글이였다. 이 글 안에 전처리시 사용한 키워드들이 모두 포함되어 있어서 삭제되지 않았음.\n",
    "\n",
    "    - 글자수가 너무 짧은 경우 : 내용이 담겨있지 않을 수 있다.\n",
    "        - ex. \"밀크티 저는 비추천이요~\" \n",
    "            - 비추천하는 이유가 담겨있지 않음.\n",
    "            - 단순 긍/부정적 키워드 판별에는 쓰일 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_check(text):\n",
    "    if len(text) < 10:\n",
    "        return 'Error : 문장짧음'\n",
    "\n",
    "    elif len(text) > 100000:\n",
    "        return 'Error : 문장이 너무 김'\n",
    "\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 글자 형태\n",
    "    - 같은 문장이 여러번 쓰이는 경우 : 광고성 글일 가능성이 있다. (XX동 XX학원 XXX선생입니다. 저희 학원에서는~)\n",
    "\n",
    "    ```문장 유사도 검출 함수로 사용하면 될듯 어떤거 쓸지는 아직 못정함...```<br>\n",
    "    ```해당 함수를 사용하여 중복 글 제거에도 사용할 수 있음```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 품사 태깅\n",
    "    - Konlpy.pos를 통해 불필요한 품사(조사 등)을 제거\n",
    "        - 체언(명사,대명사, 수사)만 남기고 제거, 필요에 따라 용언(중 동사, 형용사)도 남김."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어 찾기 함수\n",
    "\n",
    "def subject_extraction_check(text):\n",
    "    \n",
    "    # 형태소 분석\n",
    "    pos = Kkma.pos(text)\n",
    "    # pos = Mecab.pos(text)\n",
    "    # pos = Okt.pos(text)\n",
    "    # pos = Hannanum.pos(text)\n",
    "    # pos = Komoran.pos(text)\n",
    "\n",
    "    # 주어 추출\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i][1] == 'NNG':  # 명사\n",
    "            if i == 0 or (i > 0 and pos[i-1][1] != 'NNG'):  # 주어는 문장 첫 단어이거나 앞 단어가 조사가 아닌 경우\n",
    "                return pos[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사, 동사 추출\n",
    "# dictionary를 받아서 dictionary로 리턴해줌\n",
    "# side project에서 '밀크티'와 '홈런' 비교를 하느라 이렇게 만들어서 참고해주세요\n",
    "\n",
    "def extract_check(dict): \n",
    "    # dictionary 형태를 받아서 key값을 받아옴\n",
    "    key1 = list(dict.keys())[0]\n",
    "    key2 = list(dict.keys())[1]\n",
    "\n",
    "    key1_list = []\n",
    "    key2_list = []\n",
    "\n",
    "    for i in range(len(dict[key1])):\n",
    "        pos = Kkma.pos(dict[key1][i])\n",
    "        for j in pos:\n",
    "            if j[1] == 'VA': #동사\n",
    "                if j[1] not in stop_words: # 이 경우는 토큰화 할 때 stop_words 사용함\n",
    "                    # Kkma의 경우 용언의 의미부분만 추출해서 ('먹었어요' -> '먹'으로 추출) '다'를 붙여줌\n",
    "                    key1_list.append(j[0]+'다') \n",
    "            if j[1] == 'NNG': #명사\n",
    "                if j[1] not in stop_words:\n",
    "                    key1_list.append(j[0])\n",
    "\n",
    "    for i in range(len(dict[key2])):\n",
    "        pos = Kkma.pos(dict[key2][i])\n",
    "        for j in pos:\n",
    "            if j[1] == 'VA': #동사\n",
    "                if j[1] not in stop_words:\n",
    "                    key2_list.append(j[0]+'다')\n",
    "            if j[1] == 'NNG': #명사\n",
    "                if j[1] not in stop_words:\n",
    "                    key2_list.append(j[0])\n",
    "\n",
    "    frequncy_dict = {key1 : key1_list, key2 : key2_list}\n",
    "\n",
    "    return frequncy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. 띄어쓰기 구분\n",
    "    - 한글, 특히 온라인에 게시된 글들은 띄어쓰기가 올바르게 되어있지 않을 확률이 높기에 구분 짓는 방법 필요\n",
    "        - Mecab이나 Hannanum등의 형태소 분석기는 띄어쓰기를 잘 구분하지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. 분석에 불필요한 문장 제거\n",
    "    - ?!?!?!?!?!?!?! - 해봐야 알 듯 해서 아직 미작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. 기준 설정\n",
    "    1. 기간 : 최근 n개월~\n",
    "    1. 광고 : \n",
    "        - 광고 시 들어가는 문구인 `본 포스팅(이 글)은 해당 업체로부터 제품&원고료를 지원받아 작성한 리뷰입니다.`의 존재 여부 확인\n",
    "            - 이미지로 존재할 경우, 이미지 text화(library : tesseract)를 통해 확인\n",
    "\n",
    "    1. 키워드 : \n",
    "        1. 검색 시 키워드 설정\n",
    "            - 초등 `@@@` :` @@@` = [`태블릿`, `온라인 학습`, `자기주도학습`, `선행학습`, ...]\n",
    "            - `@@@` 밀크티 : `@@@` = [`초등`, `중등`, ...]\n",
    "\n",
    "        1. 분석&시각화 키워드 설정\n",
    "            - 주제에 부합한 키워드(하이퍼파라미터.. HP) 설정\n",
    "                - `컨텐츠`가 주제일 경우 HP를 ['게임', '독서', ...]로 설정\n",
    "        \n",
    "        1. 중요도 & 긍/부정 여부\n",
    "            - 중요도 : LDA등의 토픽 모델\n",
    "            - 긍/부정 : Sentiment_Analysis\n",
    "            - 아직 실습해보지 못해서 잘 모름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "진행중~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<컨텐츠>\n",
    "게임 : 게임이라는 인식의 글이 많음. 아이 입장에서는 게임에 관심, 부모님은 내키지 않음\n",
    "\t- 밀크티 홈페이지에 오각형 parameter(스토리,멀티미디어,마인드맵,동영상강의,게임)으로 나타나 있어서 고학년으로 갈 수록 게임의 비중이 낮아지는 걸 나타내지만 부모님들은 당장이 싫은건가...? 그럼 왜 필요한지...? 음...\n",
    "\n",
    "교재 : 무료 체험 시 교재가 오지 않아서 소비자가 교재에 대한 정보획득이 어렵다.\n",
    "\n",
    "양 : 컨텐츠의 절대적인 양과 사용 가능한 컨텐츠(밀크T나 윙크의 경우엔 다음 차시 학습이 잠겨있다.)\n",
    "\n",
    "<과목>\n",
    "영어 : 애니메이션의 양 + 언어 선택의 유무, \n",
    "\n",
    "독서 : \n",
    "\n",
    "<보상>\n",
    "보상 : 밀크티 : 컵(기프티콘 교환)\n",
    "\n",
    "<평가>\n",
    "\n",
    "<난이도>\n",
    "문제 : 문제출제의 기준이 명확하게\n",
    "\n",
    "<캐릭터>\n",
    "캐릭터 : 아이들이 좋아할만 하게 귀엽다, 너무 도배되어있는 느낌이라 산만하다.\n",
    "\n",
    "<학습>\n",
    "자기주도 : 성향에 따라서 선택을 다르게 하라는 조언. 능동적인 아이는 효용성이 높지만, 그렇지 않은 아이들은 부모님이 옆에서 케어하기 힘들어함\n",
    "\n",
    "담임교사 : 학생의 케어에 대한 이야기. 진도의 속도 등을 정해주거나 조언해주는 등의 케어에 만족하는 경우가 있음.\n",
    "\n",
    "무료체험 : 무료체험이 의사선택에 가장 큰 영향을 끼침. - 여기엔 모든 키워드가 포함될듯\n",
    "\t- 혜택?\n",
    "\n",
    "오답 : 문제에 틀리면 오답이라는 소리가 부모님 입장에서는 별로라고 생각하는 경우가 있다. 홈런의 경우에는 바로 복습...? 이건 확인을 해보자\n",
    "\n",
    "<기기>\n",
    "기기 : 갤럭시탭을 이용하기에 학습이 끝나도 태블릿의 효용성을 장점으로 꼽음. 다만 이러한 장점이 아이가 학습을 하지않고 유튜브를 보는 등의 집중력 저하에 영향을 끼칠 수 있다는 단점도 존재함.\n",
    "\n",
    "성능 : 오류발생 - 자잘한 딜레이 혹은 서버 접속 오류\n",
    "\n",
    "가격 : 강제로 비싼 값에 구매.\n",
    "\n",
    "<지속성>\n",
    "\n",
    "약정 : 약정에 대한 부담감이 있음. 밀크T의 경우 위약금이 없고(...?!) 기기 할부금만 내면 된다는 장점이 있음.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "그 외...\n",
    "<밀크T 중학>\n",
    "\t고등 입시 정보 게시\n",
    "\t교재(It북) 긍정적 평가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 파트 작성\n",
    "[[모델 및 시각화]]\n",
    "\n",
    "NLP : 문장 내 특정 위치에 출현하기 적합한 단어 예측 모델\n",
    "(ex. 초등 태블릿 학습을 고민해보려고 밀크티 무료체험을 해보니 컨텐츠가 풍부하고 아이가 즐거워해서 밀크티 학습을 ??? 하기로 했다.)\n",
    "\t- 기계 번역 : 자연스런 문장 생성\n",
    "\t- 오타 교정\n",
    "\t - 음성 인식\n",
    "\t - 검색어 추천\n",
    "\n",
    "Word Cloud : 출현 빈도가 높은 키워드 들을 구름같이 시각적으로 나타내줌\n",
    "\n",
    "Treemap : 여러개의 직사각형으로 크기와 색의 진한 정도에 따라서 데이터의 양을 나타내준다.\n",
    "\n",
    "\n",
    "SNA(Social Network Analysis : 사회 연결망 분석) : \n",
    "\t객체들 간의 관계를 시각화 표현하여 구조를 파악하기 용이하다.\n",
    "\n",
    "Cluster(군집화) : 객체(문장)의 특징이 유사한 그룹으로 군집화하여 시각화\n",
    "\t- Dendrogram : 유사한 특징을 가진 객체들끼리 토너먼트 모양의 선으로 연결\n",
    "\n",
    "TF-IDF : 단어의 빈도 수를 이용한 수치화 방법\n",
    "DTM(Document-Term Matrix) : 단어들의 빈도를 행렬로 표시한 것\n",
    "LSA(Latent Semantic Analysis : 잠재 의미 분석) : \n",
    "\tDTM, TF-IDF는 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있는데 이를 보완하여 DTM의 잠재된 의미를 이끌어 내는 방법\n",
    "LDA(Latent Dirichlet Allocation : 잠재 디리클레 할당)(TM_W2V  : 토픽모델링) : \n",
    "\t토픽의 개수를 하이퍼파라미터로 지정하여 해당 개수만큼의 토픽이 문장에 존재할 확률을 예측한다.\n",
    "\n",
    "Sentiment Analysis(감정 분석): 텍스트가 긍정적인지, 부정적인지 혹은 중립적인지 확인\n",
    "...오?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
